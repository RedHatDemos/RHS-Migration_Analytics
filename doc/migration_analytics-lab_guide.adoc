:scrollbar:
:data-uri:
:toc2:
:imagesdir: images

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

== Migration Analytics 0.1 beta - Lab Guide

:numbered:

== Introduction

This Lab is built to help better understand the status of a running environment in order to assess, plan and execute a migration from a proprietary environment to an open source one. In order to do so, there are two tools built to help obtain and process data, initially focused on the Virtualization layer, but working to introspect the Operating System as well as the Applications. 

[NOTE] 
If you are interested in migrating applications, please check the link:https://developers.redhat.com/products/rhamt/overview[Red Hat Application Migration Toolkit]

Migration Analytics uses a custom plugin within Red Hat CloudForms to examine the environment. This plugin helps obtain the data, and download it as a single file, so it can be reviewed, and then uploaded to be analyzed.

[NOTE]
Upcoming versions of Migration Analytics will be able to upload the data directly, as long as the appliance has access to internet.


In order to analyze data, we will use the Migration Analytics application running as SaaS on cloud.redhat.com. This tool will help us obtain the following reports to better understand the cost, risk and effort required to perform the migration:

* Initial Savings Estimation
* Workload Migration Summary
* Workload Migration Inventory

.Goal
* The goal of this lab is to obtain a set of data from a running environment, review it, upload it to be analyzed, and understand the benefits of the outcome provided in order to better assess and drive a migration.

== Overview

The environment to be used for this lab provides a running VMware vSphere environment with different workloads on it, and a deployed and configured Red Hat CloudForms instance, with the Migration Analytics plugin configured to obtain data from it.

In this environment the following configuration steps are already done:

* Enabled Migration Analytics plugin in CloudForms
* Added the `vSphere` virtualization provider in CloudForms
* Provide credentials for virtualization providers and hosts in CloudForms
* Configured Smart State Analysis in CloudForms
* Configured Ansible, an ansible hosts file, in `workstation` and, exchanged ssh keys (`/root/.ssh/id_rsa`) with all running machines.

Required versions of products used:

[cols="1,1",options="header"]
|=======
|Product |Version
|CloudForms |5.0.0+ 
|VMware vSphere |6.0+ (6.7 used)
|=======

The Migration Analytics analysis tool (WAT) is running as an application on the OpenShift instances that power cloud.redhat.com. 

[NOTE]
The tool uses Red Hat Decision Manager, Red Hat AMQ, Red Hat Fuse and other opensource tools to run. for more information please check link:https://project-xavier.io[project-xavier.io].

== Requirements to access and perform this lab

=== Base requirements

* A computer with access to the Internet :-)
* SSH client
* Firefox, or Chromium / Chrome

[WARNING]
In modern versions of Firefox (>60) an TLS negotiation issue may happen that will take aprox 40 seconds until timeout, before it let you access the CloudForms interface. This is due an issue with Firefox cert storage. You may create a new profile for it (with `firefox --ProfileManager`), or regenerate the cert storages by stopping it, moving `cert9.db` and `cert8.db` to an archive place and start firefox again.

[WARNING]
Grammarly plugin for Chrome is known to cause problems when managing CloudForms. If you use it, please deactivate it while doing this lab.

=== Obtaining or enabling access credentials

This lab could be performed in a classroom (whether virtual or physical), in which case the proctors running the lab will provide instructions on how to get your own instance.

If you plan to run it on a personal basis to learn, demo or simply "taste it" you may take these points into account: 

. First time login?, forgot login or password? Go to https://www.opentlc.com/account (Note, your username should NOT have an *@* in it.)

. Red Hat Partners can also request access to the Red Hat Product Demo System (link:https://rhpds.redhat.com[RHPDS]) by sending an email to open-program@redhat.com. 

. Passwords to the services: `r3dh4t1!`. If you can't access it please contact GPTE or the link:https://mojo.redhat.com/community/marketing/vertical-marketing/horizontal-solutions/people[Horizontal Solutions Team].

== Environment

A full new migration environment is deployed on every request. To make the environment unique a 4 character identifier is assigned to it (i.e. `1e37`), this identifier is referred in this documentation as *YOUR-GUID*.  

* Systems

The migration environment consists of the following systems:

image::blueprint.png[Blueprint]

[cols="1,1,1,2",options="header"]
|=======
| Hostname | Internal IP | External name | Description
|`workstation.example.com` |`192.168.0.10` | workstation-<YOUR-GUID>.rhpds.opentlc.com |Jump host and Ansible host
|`storage.example.com` |`192.168.0.254` | workstation-<YOUR-GUID>.rhpds.opentlc.com | NFS server
|`cf.example.com` |`192.168.0.100` |  cf-<YOUR-GUID>.rhpds.opentlc.com |CloudForms server
|`vcenter.example.com` |`192.168.0.50` | vcenter-<YOUR-GUID>.rhpds.opentlc.com |VMware vCenter server
|`esx1.example.com` |`192.168.0.51` | N/A |ESXi hypervisor
|`esx2.example.com` |`192.168.0.52` | N/A |ESXi hypervisor
|=======

The architecture of the Migration Analytics environment and workflow can be depicted as it follows:

image::architecture_diagram.png[Architecture Diagram]

* Networks

Networks used in the environment:

[cols="1,1,2",options="header"]
|=======
| Network Name | IP range | Description
| `Admin` | `192.168.x.x/16` | General administration and storage network.
| `Service` | `10.10.0.x/24` | Internal network for the app to connect LB to EAP and to DB. 
| `Service-DMZ` | `10.9.0.x/24` | External DMZ network to publish the app. Also access to the user API for OSP and Horizon (provider network)
|=======

* Virtual Machines 

This deployment of the migration environment includes the following VMs provisioned in the vSphere environment in order to be migrated:

[cols="1,1,2",options="header"]
|=======
| Name | IPs | Description
| `jboss0.example.com` | 10.10.0.110 | Red Hat Enterprise Linux 7 host running JBoss EAP, connected to the `Service` network for ticket-monster.
| `jboss1.example.com` | 10.10.0.111 | Red Hat Enterprise Linux 7 host running JBoss EAP, connected to the `Service` network for ticket-monster.
| `lb.example.com` | 10.10.0.100 , 10.9.0.100 | Red Hat Enterprise Linux 7 host running JBoss Core Service Apache HTTP server configured with mod_cluster to proxy traffic to `jboss0` and `jboss1`, connected to the `Service` and `Servicer-DMZ` networks for ticket-monster.
| `db.example.com` | 10.10.0.120 | Red Hat Enterprise Linux 7 host running PostgreSQL providing service to `jboss0` and `jboss1` through the `Service` network for ticket-monster.
| `freebsd.example.com` | 10.10.0.100 | FreeBSD 12 connected through the `Service` network.
| `hana.example.com` | 10.10.0.150 | Red Hat Enterprise Linux 7 SAP HANA Express through the `Service` network.
| `tomcat.example.com` | 10.10.0.180 | CentOS 7 host running Apache Tomcat 8 server through the `Service` network.
| `weblogic.example.com` | 10.10.0.181 | Red Hat Enterprise Linux 7 host running Oracle Weblogic 12 server through the `Service` network.
| `websphere.example.com` | 10.10.0.182 | Red Hat Enterprise Linux 7 host running IBM WebSphere 8 server through the `Service` network.
| `oracledb.example.com` | 10.10.0.160 | CentOS 7 host running Apache Tomcat 8 server through the `Service` network.
| `msssql.example.com` | 10.10.0.190 | Red Hat Enterprise Linux 7 host running Microsoft SQL server through the `Service` network.
|=======

== Getting Started

=== Accessing the environment

**RHTE**

. Once the environment is up and running, and we have it assigned to ourselves, we use SSH to test access to it, by connecting to the `workstation`.  The SSH path is provided in the GUID grabber tool.
+
----
$ ssh lab-user@workstation-<YOUR-GUID>.rhpds.opentlc.com
----
+
. Once you check that you can connect to workstation, become `root` using the provided password:
+
----
$ sudo -i
----

Now that you have accessed the `workstation` machine and become `root`, you can check the rest of the infrastructure.


**RHPDS**

. Once the environment is up and running, and we have it assigned to ourselves, we use SSH to test access to it, by connecting to the `workstation` using your OPENTLC login name and private SSH key.

* Using a Unix/Linux system:
+
----
$ ssh -i /path/to/private_key <YOUR-OpenTLC-USERNAME-redhat.com>@workstation-<YOUR-GUID>.rhpds.opentlc.com
----

* Example for user 'batman' and GUID '1e37', using the default ssh private key:
+
----
$ ssh -i ~/.ssh/id_rsa batman-redhat.com@workstation-1e37.rhpds.opentlc.com
Last login: Mon Aug 26 05:03:34 2019 from workstation.example.com
[batman-redhat.com@workstation-1e37 ~]$ 
----

. Once you check that you can connect to workstation, become `root` using the provided password:
+
----
$ sudo -i
----

Now that you have accessed the `workstation` machine and become `root`, you can check the rest of the infrastructure.

=== Checking infrastructure status

. Check the status of the infrastructure running the environment, from the `workstation`, using ansible:
+
----
# ansible infra -m ping
----
+
This command establishes a SSH connection to all the infrastructure machines in the environment, defined as `infra` in `/etc/ansible/hosts` which are: vCenter and ESXi servers, storage and workstation, as well as CloudForms. If the machines are being built of booted up, they will show as unreachable. In case the machines are up an running a success message, per each, will show up. 
This is an example of a success message for the VM `cf.example.com`:
+
----
cf.example.com | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
----
+ 
[WARNING]
As this environment is generated and powered up for you in a cloud environment, some resources may suffer from issues or delays depending on the status of the cloud its running on. You may need to wait until everything is up and running, and manually start up or reboot some of them. Follow carefully the upcoming steps to ensure your lab is in a proper running status.

. Let's manually check that CloudForms is running by establishing an SSH connection to it (from workstation) and take a look at `automation.log`:
+
----
[root@workstation-repl ~]# ssh cf
Welcome to the Appliance Console

For a menu, please type: appliance_console
Web console: https://cf.example.com:9090/ or https://192.168.0.100:9090/

Last login: Mon Aug 26 07:08:05 2019 from 192.168.0.10
[root@cf ~]# tail -f /var/www/miq/vmdb/log/automation.log
----
+
[TIP]
The log entries are very long, so it helps if you stretch this window as wide as possible.

Before checking the vSphere environment we have to set up a tunnel to the running environment.
+
[WARNING]
You must log out from the SSH session to go to the next section.

=== Creating an SSH tunnel to the Environment

To access all the resources, from our browser, the same way we would do it being connected directly to the management network, we are going to create an SSH tunnel from our working laptop to the `workstation` machine. This is the diagram of how it will work (explained below):
 
image::ssh_tunnel.png[SSH tunnel for Proxy]

* Our `laptop` connects to `workstation` via ssh
* SSH is instructed to listen on `localhost:3128` in the `laptop`
* SSH takes all the traffic from `localhost:3128` in the `laptop` to `localhost:3128` in the `workstation`
* There is a squid proxy service listening in `localhost:3128` in the `workstation`
* The browser in the `laptop` is configured to use the proxy in `localhost:3128` ... and all the traffic will be sent to the squid proxy in the `workstation`, including the DNS queries. 
* The browser can point now to any service using the internal name (i.e. https://vcenter.example.com ) ... let's do it!

Time to move ahead.

**Workshop**

. Let's fire up SSH in your workstation but this time with the "tunnel" option `-L localhost:3128:localhost:3128`
+
----
$ ssh -L localhost:3128:localhost:3128 lab-user@workstation-<YOUR-GUID>.rhpds.opentlc.com
----

.  Running it shall simply provide a shell prompt. This an example on how it would look like:
+
----
$ ssh -L localhost:3128:localhost:3128 lab-user@workstation-<YOUR-GUID>.rhpds.opentlc.com
Last login: Mon Aug 26 05:03:34 2019 from workstation.example.com
[lab-user@workstation-1e37 ~]$ 
----

**RHPDS**

. Let's fire up SSH in your workstation but this time with the "tunnel" option `-L localhost:3128:localhost:3128`
+
----
$ ssh -i /path/to/private_key -L localhost:3128:localhost:3128 <YOUR-OpenTLC-USERNAME-redhat.com>@workstation-<YOUR-GUID>.rhpds.opentlc.com
----

.  Running it shall simply provide a shell prompt. This an example on how it would look like:
+
----
$ ssh -i ~/.ssh/id_rsa -L localhost:3128:localhost:3128 batman-redhat.com@workstation-1e37.rhpds.opentlc.com
Last login: Mon Aug 26 05:03:34 2019 from workstation.example.com
[batman-redhat.com@workstation-1e37 ~]$ 
----

**COMMON**

. Now by configuring the browser to access proxy in `localhost` port `3128` for all protocols, we will be running it as if it was directly inside the environment, consuming the internal DNS names. Internal DNS names used the domain `example.com`. Configure the proxy, for this use case, following this example:
+
image::localhost_proxy_config.png[Localhost Proxy Config]

. Time to point our browser to an internal URL ... http://vcenter.example.com
+
image::firefox_ssh_tunnel_vcenter.png[Firefox accessing vCenter using a tunnel]
+
[WARNING]
If you haven't managed to make this work, please do not hesitate asking for help. It will be key to proceed with the rest of the lab.

=== Checking vSphere status

. The vSphere environment has been instantiated for us in the cloud and we will be using *nested virtualization* so, the performance may not be as good as in a full baremetal environment. Let's login in the WebUI:
+
image::vsphere_checks_01.png[Access vCenter UI]

. Use the administrator username which is `administrator@vsphere.local` and the provided password.
+
image::vsphere_checks_02.png[Login in SSO UI]

. Once in the vCenter UI, click on the *Hosts and Clusters* icon (1), then select *VMCluster* (2) and last, *Reset to Green* all the warnings shown
+
image::vsphere_checks_03.png[Reset to Green]

. VMs should have been started by the `start_vms` script in `workstation`. In case some VMs are stopped you may manually start them by selecting it and clicking on the *play* icon
+
image::vsphere_checks_04.png[Press play]
+
[NOTE]
you can also check the status of the `start_vms` script by looking at the ansible log in `workstation`
+
----
# tail -f /var/log/ansible.log
----

Now the environment is ready to move ahead. We can verify that the VMs in vSphere are up by checking that the Ticket Monster app is running:

* Point your browser to  http://app.example.com and check it is running:
+
image::app-ticketmonster-running.png[Ticket Monster app running]
[NOTE]
You must accept all of the self-signed SSL certificates.
+
image::ssl_cert_warning.png[SSL Cert Warning]

== Examining VMware environment with Migration Analytics

The *Workload Examination Toolkit* is being built within CloudForms to help examine the VMware environment and provide a set of data to analyze it. The initial steps will be taken with a preconfigured environment, which later on will be reset to a "just deployed" state to practice how to configure it.

. Access CloudForms through it's URL http://cf.example.com and login using `admin` username and the provided password.
+
image::cloudforms_login.png[CloudForms Login]

. Go to *Compute -> Infrastructure -> Providers*
+
image::cloudforms_check_virtualization_provider_01.png[Check Virt Provider]

. Select *vSphere* and click on *Authentication -> Re-Check Authentication Status*. This will confirm that the credentials work with the current infra.
+
image::cloudforms_check_virtualization_provider_02.png[Check Virt Provider]

. While the check is being performed, go to *Configuration -> Refresh Relationships and Power States* to get a fresh status of the running VMs
+
image::cloudforms_check_virtualization_provider_03.png[Check Virt Provider]
+
image::cloudforms_check_virtualization_provider_04.png[Check Virt Provider]

. Now that the infrastructure is completely refreshed, we will extract data from it. Go to menu *Migration -> Migration Analytics*. 
+
image::migration_analytics_examination_01.png[Migration Analytics Examination]

. Click on *Get started* button
+
image::migration_analytics_examination_02.png[Migration Analytics Examination]

. After the plugin has checked providers you will be taken to a summary page with all available providers. If you click on *vSphere* you will see a summary data of that provider. 
+
image::migration_analytics_examination_03.png[Migration Analytics Examination]

. Now let's click on *Collect inventory data*
+
image::migration_analytics_examination_04.png[Migration Analytics Examination]
+
image::migration_analytics_examination_05.png[Migration Analytics Examination]

. Select Provider *vSphere*
+
image::migration_analytics_examination_06.png[Migration Analytics Examination]

. Select *Detailed data*, as the current appliance is configured to run Smart State Analysis. Click *Continue*
+
image::migration_analytics_examination_07.png[Migration Analytics Examination]

. Inventory collection will be complete. A file has been delivered in the filesystem of `cf.example.com`, in this case is the file `/tmp/cfme_inventory-20190829-2305-mvl6xy.tar.gz`. We will copy it manually. We may *return to summary*

To copy the file to you local machine we first must copy it to the bastion host:
+
----
$ scp root@192.168.0.100:/tmp/cfme_inventory-20190904-2341-1h3vafb.tar.gz .
----

Then we need to copy the file to our local machine:
+
----
$ scp lab-user@workstation-<GUID>.rhpds.opentlc.com:/home/lab-user/cfme_inventory-20190904-2341-1h3vafb.tar.gz .
----

image::migration_analytics_examination_08.png[Migration Analytics Examination]

=== Analyzing the Data from Examination with the Workload Analysis Tool

Once we have the data we proceed to analyze it in our Software as a Service offering, the Workload Analysis Tool, which is part of cloud.redhat.com 

. We access https://cloud.redhat.com/beta (until it's available, we will use an internal sandbox environment, please ask for help in migrate@redhat.com if you want access). We select in *Migration Services*, the entry *Migraion Analytics*
+
image::migration_analytics_WAT_01.png[Workload Analysis Tool]

. Once in the *Migration Analytics* app, we click on *Create*
+
image::migration_analytics_WAT_02.png[Workload Analysis Tool]

. We select the payload file to be analyzed. We may want to use this link:https://github.com/RedHatDemos/RHS-Migration_Analytics/blob/master/payloads/cfme_inventory-20190912-demolab_withSSA.tar.gz?raw=true[sample file] that contains more hosts and VMs than the lab environment. We choose our *Report Name* and provide a *Report Description*. It is important ot include the growth rate expected, as well as the planned migration per year.
+
image::migration_analytics_WAT_03.png[Workload Analysis Tool]

. The file gets uploaded
+
image::migration_analytics_WAT_04.png[Workload Analysis Tool]

. A new entry appears while the reports are being generated.
+
image::migration_analytics_WAT_05.png[Workload Analysis Tool]

. Once the report is created we can click on its name to access it
+
image::migration_analytics_WAT_06.png[Workload Analysis Tool]

. The first thing we see is the *Initial Savings Estimation* report with a set of numbers providing a high level approach to the expected savings based on the data retrieved from the infrastructure.
+
image::migration_analytics_WAT_07.png[Workload Analysis Tool]

. Clicking on *Workload Migration Summary* we can see (TODO) a report providing information on the efforts required to perform the migrations as well as more detailed data on the workloads to help plan further.
+
image::migration_analytics_WAT_08.png[Workload Analysis Tool]

. Clicking on *Workload Migration Inventory* provides a detailed list of the Virtual Machines to be migrated, with information on the workload, the operating system, as well as the estimated effort and recommended targets. This will help proceed with the detailed plan of the migration
+
image::migration_analytics_WAT_09.png[Workload Analysis Tool]


== Configuring CloudForms to run as Migration Analytics Workload Examination Toolkit

To deploy a CloudForms appliance on VMware you may want to follow link:https://access.redhat.com/documentation/en-us/red_hat_cloudforms/5.0-beta/html/installing_red_hat_cloudforms_on_red_hat_virtualization/index[the official CloudForms documentation].

=== CloudForms Reset

Once the overview is done, we can proceed by SSH to the `workstation`. 

We continue by running, in `workstation`, the playbook to unconfigure the deployed CloudForms:

----
# cd /root/RHS-Migration_Analytics/playbooks/
# ansible-playbook unconfigure_cf.yml
----

. The playbook will stop the CloudForms services, will reset the database, and restart the services.
+
image::reset_cloudforms_01.png[Launch CF reset playbook]

. After CloudForms database reset, the users will be removed and the `admin` will have the *password reset* to the default appliance password (*smartvm*). We shall change that default password to the provided one by clicking in `update password` in the CloudForms login screen and filling up the new password fields:

=== Add vSphere Provider

We will login in CloudForms using the appliance default credentials, being the user `admin` and using the default appliance password (*smartvm*).

image::add_vsphere_provider_01.png[Add vSphere provider to CloudForms]

. We click on *Add provider*. If the menu doesn't show up, we can go there by clicking on *Compute -> Infrastructure -> Providers*.
+
image::add_vsphere_provider_02.png[Add vSphere provider to CloudForms]

. We add all the data for the new infrastructure provider:
+
* Name: "vSphere"
* Type: "VMware vCenter"
* Hostname: "vcenter.example.com"
* Username: "administrator@vsphere.local"
* Password: ... the provided one
+
image::add_vsphere_provider_03.png[Add vSphere provider to CloudForms]

. Once we click on *validate* we can ensure that the credentials are OK and we can proceed to click the *Add* at the bottom of the page.
+
image::add_vsphere_provider_04.png[Add vSphere provider to CloudForms]

. We reach to the point where the *Insfrastructure Provider* has been added
+
image::add_vsphere_provider_05.png[Add vSphere provider to CloudForms]

. Now we shall add the credentials to access hosts. We go to *Compute -> Infrastructure -> Hosts*
+
image::add_vsphere_provider_06.png[Add vSphere provider to CloudForms]

. We select both hosts and click on *Configuration -> Edit Selected Items*
+
image::add_vsphere_provider_07.png[Add vSphere provider to CloudForms]

. Then we add the credentials being *Username* `root` and the provided password.
+
image::add_vsphere_provider_08.png[Add vSphere provider to CloudForms]

. We get the credentials saved successfuly
+
image::add_vsphere_provider_09.png[Add vSphere provider to CloudForms]

[WARNING]
If it's not possible to add credentials for the ESXi hosts (or there are too many hosts with different credentials), VM scanning can still be performed using an authentication token provided by the vCenter. To configure this, set scan_via_host to be false in Configuration → Advanced settings:
+
----
:coresident_miqproxy:
  :scan_via_host: false
----

=== Configure SmartState Analysis
[WARNING]
Since the appliance was already previously configured for Smart State Analysis, the vddk library and associated tasks with it exist. The steps below are used when configuring a brand new appliance.

. Install the VMware VDDK onto the CFME appliance. First copy the *VMware-vix-disklib* from the `workstation` folder to `cf`:
+
----
[root@workstation-repl ~]# scp /root/Downloads/VMware-vix-disklib-stable.tar.gz cf:/root/
VMware-vix-disklib-stable.tar.gz              100%   19MB  86.0MB/s   00:00 
----

. SSH into `cf` and *untar* the file
+
----
[root@cf-REPL ~]# tar xzvf VMware-vix-disklib-stable.tar.gz
[Output removed]
----

. Create lib folder and copy content there:
+
----
[root@cf-REPL ~]# mkdir -p /usr/lib/vmware-vix-disklib
[root@cf-REPL ~]# cd vmware-vix-disklib-distrib
[root@cf-REPL vmware-vix-disklib-distrib]# cp -r bin64 /usr/lib/vmware-vix-disklib/
[root@cf-REPL vmware-vix-disklib-distrib]# cp -r lib64 /usr/lib/vmware-vix-disklib/
[root@cf-REPL vmware-vix-disklib-distrib]# cp -r include /usr/lib/vmware-vix-disklib/
----

. Create symbolic links to make them available and load them:
+
----
[root@cf-REPL ~]# ln -s /usr/lib/vmware-vix-disklib/lib64/libvixDiskLib.so /usr/lib/libvixDiskLib.so
[root@cf-REPL ~]# ln -s /usr/lib/vmware-vix-disklib/lib64/libvixDiskLib.so.6 /usr/lib/libvixDiskLib.so.6
[root@cf-REPL ~]# ldconfig
----

. Check they are added correctly
+
----
[root@cf-REPL ~]# ldconfig -p | grep vix
        libvixDiskLib.so.6 (libc6,x86-64) => /lib/libvixDiskLib.so.6
        libvixDiskLib.so (libc6,x86-64) => /lib/libvixDiskLib.so
----

. Now, we have to configure Smart State Analysis in CloudForms. We move to the UI in http://cf.example.com 

. Check that both the SmartProxy and SmartState Analysis workers are enabled (on). Click on the *gear icon* (Configuration) on the top right corner of the UI, and then go to *Settings -> Server*:
+
image::cloudforms-configure_ssa-03.png[Configure Smart State Analysis 3]

. Check in *Configuration -> Settings* (remember, Configuration is the gear in the top right corner) that there is no `default` analysis profile. You may examine the `sample` analysis profile:
+
image::cloudforms-configure_ssa-04.png[Configure Smart State Analysis 4]
+
[NOTE]
In case you have a default profile, you may need to rename it, or remove it.

. Now go to the cloudforms commandline in `cf`
+
----
# ssh cf
----

. Obtain a preconfigured export of the `default` profile. We provide this one for you:
+
----
# curl https://raw.githubusercontent.com/RedHatDemos/RHS-Migration_Analytics/master/config/default_SSA_profile.yaml > /root/default_SSA_profile.yaml
----
+
[NOTE]
This profile can be created by exporting the `sample` profile and editing it.

. Import the `default` profile
+
----
[root@cf-REPL ~]# vmdb
[root@cf-REPL vmdb]# pwd
/var/www/miq/vmdb
[root@cf-REPL vmdb]# bundle exec rake evm:import:scan_profiles -- --source /root/default_SSA_profile.yaml
----

. Now you may want to go to *Configuration -> Settings* to review the new profile `default` (A reload of the page could be enough):
+
image::cloudforms-configure_ssa-05.png[Configure Smart State Analysis 5]

. Now you may want to edit the `default` pforile and check the `File` tab of it:
+
image::cloudforms-configure_ssa-06.png[Configure Smart State Analysis 6]
+
image::cloudforms-configure_ssa-06b.png[Configure Smart State Analysis 6]
+

. Go to *Compute -> Infrastructure -> Providers*. Select the `vSphere` provider and then go to *Policy -> Manage Policies*
+
image::cloudforms-configure_ssa-07.png[Configure Smart State Analysis 7]

. Select *VM SmartState Analysis profile* to add it to the the VMware `vSphere` provider `vSphere`. Click *Save*:
+
image::cloudforms-configure_ssa-08.png[Configure Smart State Analysis 8]
+
[NOTE]
When a control policy has been successfully added to the provider a gold ‘shield’ icon will appear on the provider’s tile icon.

. Tag any VMs that shouldn’t be analyzed with the exclusions/do_not_analyze tag (Mainly Windows VMs running stateless applications such as Exchange server, possibly other databases). Snapshotting such VMs may cause data corruption.

. Optionally (for large environments)  increase the following values in Configuration -> Advanced settings to allow for more concurrent scans:
+
----
:coresident_miqproxy:
  :concurrent_per_ems: 1
  :concurrent_per_host: 1
----
+
. Optionally (for very large multi-1000 VM environments) increase the number of VM Analysis Collector workers, or add further appliances and set the SmartProxy Affinity to delegate scanning of certain Hosts & Datastores to specific CFME appliances. 

.. For a one-off scan, select one/several/all VMs in the Compute -> Infrastructure -> Virtual Machines page and click Configuration -> Perform SmartState Analysis

.. For a scheduled scan, add a “VM Analysis” schedule to Configuration -> Settings -> Schedules

. Monitor the status of the scans in the <Username> → Tasks page of the WebUI, and/or using the last_scan_on, last_scan_attempt_on and last_sync_on attributes on a VM object. Scanning can take up to 1 minute per VM. The "VM SmartState Analysis profile” should also tag VMs with operations/analysis_failed or operations/analysis_success as appropriate. 
+
When an SSA of a VM has completed successfully, the VM details page will have more information, i.e.
+
image::cloudforms-configure_ssa-09.png[Configure Smart State Analysis 9]

=== Enable Migration Analytics Menu

. In the CloudForms UI, navigate to Settings using the ⚙️ icon in the upper-right

. Click the Advanced tab. Find the `prototype` section, and under `migration_analytics` in this section, change `enabled` from false to true
```diff
 :prototype:
   :migration_analytics:
-    :enabled: false
+    :enabled: true
```

. Click Save

. SSH in the Cloudforms VM
----
# ssh cloudforms.example.com
----

. Restart the service
----
# systemctl restart evmserverd.service
----

=== Run new examination

Now the environment is ready to run another examination.


