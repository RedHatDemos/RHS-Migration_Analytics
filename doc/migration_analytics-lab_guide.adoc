:scrollbar:
:data-uri:
:toc2:
:imagesdir: images

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

== Migration Analytics 0.1 beta - Lab Guide

:numbered:

== Introduction

This Lab is built to help better understand the status of a running environment in order to assess, plan and execute a migration from a proprietary environment to an open source one. In order to do so, there are two tools built to help obtain and process data, initially focused on the Virtualization layer, but working to introspect the Operating System as well as the Applications. 
+
[NOTE] 
If you are interested in migrating applications, please check the link:https://developers.redhat.com/products/rhamt/overview[Red Hat Application Migration Toolkit]

The Lab Environment uses, to obtain data, the *Workload Examination Toolkit* (a.k.a. WET) which is a Migration Analytics plugin within Red Hat CloudForms. This toolkit help obtain the data, and download it as a single file, so it can be reviewed, and then uploaded to be analyzed.
+
[NOTE]
Updoming versions of Migration Analytics will be able to upload the data directly, as long as the appliance has access to internet.


In order to analyze data, we will use the *Workload Analysis Tool* (a.k.a. WAT), which is an application running as SaaS on cloud.redhat.com. This tool will help us obtain the following reports to better understand the cost, risk and effort required to perform the migration:

* Initial Savings Estimation
* Workload Migration Summary
* Workload Migration Inventory

.Goal
* the goal of this lab is to obtain a set of data from a running environment, review it, upload it to be analyzed, and understand the benefits of the outcome provided in order to better assess and drive a migration.

== Overview

The environment to be used for this lab provides a running VMware vSphere environment with different workloads on it, and a deployed and configured Red Hat CloudForms instance, with the Migration Analytics plugin configured to obtain data from it.

In this environment the following configuration steps are already done:

* Enabled Migration Analytics plugin in CloudForms
* Added the `vSphere` virtualization provider in CloudForms
* Provide credentials for virtualization providers and hosts in CloudForms
* Configured Smart State Analysis in CloudForms
* Configured Ansible, an ansible hosts file, in `workstation` andi, exchanged ssh keys (`/root/.ssh/id_rsa`) with all running machines.

The password to access all services is available link:https://mojo.redhat.com/docs/DOC-1174612-accessing-red-hat-solutions-lab-in-rhpds[here]

Required versions of products used:

[cols="1,1",options="header"]
|=======
|Product |Version
|CloudForms |5.0.0+ 
|VMware vSphere |5.5+ (6.7 used)
|=======

The Migration Analytics analysis tool (WAT) is running as an application on the OpenShift instances that power cloud.redhat.com. 
+
[NOTE]
The tool uses Red Hat Decision Manager, Red Hat AMQ, Red Hat Fuse and other opensource tools to run. for more information please check link:https://project-xavier.io[project-xavier.io].

== Requirements to access and perform this lab

=== Base requirements

* A computer with access to the Internet :-)
* SSH client
* Firefox, or Chromium / Chrome
+
[WARNING]
In modern versions of Firefox (>60) an TLS negotiation issue may happen that will take aprox 40 seconds until timeout, before it let you access the CloudForms interface. This is due an issue with Firefox cert storage. You may create a new profile for it (with `firefox --ProfileManager`), or regenerate the cert storages by stopping it, moving `cert9.db` and `cert8.db` to an archive place and start firefox again.
+
[WARNING]
Grammarly plugin for Chrome is known to cause problems when managing CloudForms. If you use it, please deactivate it while doing this lab.

=== Obtaining or enabling access credentials

This lab coud be performed in a classroom (whether virtual or physical), in which case the proctors running the lab will provide instructions on how to get your own instance.

If you plan to run it on a personal basis to learn, demo or simply "taste it" you may take these points into account: 

. First time login?, forgot login or password? Go to https://www.opentlc.com/account (Note, your username should NOT have an *@* in it.)

. Red Hat Partners can also request access to the Red Hat Product Demo System (link:https://rhpds.redhat.com[RHPDS]) by sending an email to open-program@redhat.com. 

. Passwords to the services are referred as `<to_be_provided>`. The credentials to access all services are available link:https://mojo.redhat.com/docs/DOC-1174612-accessing-red-hat-solutions-lab-in-rhpds[here]. If you can't access it please contact GPTE or the link:https://mojo.redhat.com/community/marketing/vertical-marketing/horizontal-solutions/people[Horizontal Solutions Team].

== Environment

A full new migration environment is deployed on every request. To make the environment unique a 4 character identifier is assigned to it (i.e. `1e37`), this identifier is referred in this documentation as *YOUR-GUID*.  

* Systems
The migration environment consists of the following systems:

image::blueprint.png[Blueprint]

[cols="1,1,1,2",options="header"]
|=======
| Hostname | Internal IP | External name | Description
|`workstation.example.com` |`192.168.0.10` | workstation-<YOUR-GUID>.rhpds.opentlc.com |Jump host and Ansible host
|`storage.example.com` |`192.168.0.254` | workstation-<YOUR-GUID>.rhpds.opentlc.com | NFS server
|`cf.example.com` |`192.168.0.100` |  cf-<YOUR-GUID>.rhpds.opentlc.com |CloudForms server
|`vcenter.example.com` |`192.168.0.50` | vcenter-<YOUR-GUID>.rhpds.opentlc.com |VMware vCenter server
|`esx1.example.com` |`192.168.0.51` | N/A |ESXi hypervisor
|`esx2.example.com` |`192.168.0.52` | N/A |ESXi hypervisor
|=======

The architecture of the Migration Analytics environment and workflow can be depicted as it follows:

image::architecture_diagram.png[Architecture Diagram]

* Networks
Networks used in the environment

[cols="1,1,2",options="header"]
|=======
| Network Name | IP range | Description
| `Admin` | `192.168.x.x/16` | General administration and storage network.
| `Service` | `10.10.0.x/24` | Internal network for the app to connect LB to EAP and to DB. 
| `Service-DMZ` | `10.9.0.x/24` | External DMZ network to publish the app. Also access to the user API for OSP and Horizon (provider network)
|=======

* Virtual Machines 
This deployment of the migration environment includes the following VMs provisioned in the vSphere environment in order to be migrated:

[cols="1,1,2",options="header"]
|=======
| Name | IPs | Description
| `jboss0.example.com` | 10.10.0.110 | Red Hat Enterprise Linux 7 host running JBoss EAP, connected to the `Service` network for ticket-monster.
| `jboss1.example.com` | 10.10.0.111 | Red Hat Enterprise Linux 7 host running JBoss EAP, connected to the `Service` network for ticket-monster.
| `lb.example.com` | 10.10.0.100 , 10.9.0.100 | Red Hat Enterprise Linux 7 host running JBoss Core Service Apache HTTP server configured with mod_cluster to proxy traffic to `jboss0` and `jboss1`, connected to the `Service` and `Servicer-DMZ` networks for ticket-monster.
| `db.example.com` | 10.10.0.120 | Red Hat Enterprise Linux 7 host running PostgreSQL providing service to `jboss0` and `jboss1` through the `Service` network for ticket-monster.
| `freebsd.example.com` | 10.10.0.100 | FreeBSD 12 connected through the `Service` network.
| `hana.example.com` | 10.10.0.150 | Red Hat Enterprise Linux 7 SAP HANA Express through the `Service` network.
| `tomcat.example.com` | 10.10.0.180 | CentOS 7 host running Apache Tomcat 8 server through the `Service` network.
| `weblogic.example.com` | 10.10.0.181 | Red Hat Enterprise Linux 7 host running Oracle Weblogic 12 server through the `Service` network.
| `websphere.example.com` | 10.10.0.182 | Red Hat Enterprise Linux 7 host running IBM WebSphere 8 server through the `Service` network.
| `oracledb.example.com` | 10.10.0.160 | CentOS 7 host running Apache Tomcat 8 server through the `Service` network.
| `msssql.example.com` | 10.10.0.190 | Red Hat Enterprise Linux 7 host running Microsoft SQL server through the `Service` network.
|=======

== Getting Started

=== Accessing the environment

. Once the environment is up and running, and we have it assigned to ourseleve, we use SSH to test access to it, by connecting to the `workstation` using your OPENTLC login name and private SSH key.

* Using a Unix/Linux system:
+
----
$ ssh -i /path/to/private_key <YOUR-OpenTLC-USERNAME-redhat.com>@workstation-<YOUR-GUID>.rhpds.opentlc.com
----

* Example for user 'batman' and GUID '1e37', using the default ssh private key:
+
----
$ ssh -i ~/.ssh/id_rsa batman-redhat.com@workstation-1e37.rhpds.opentlc.com
Last login: Mon Aug 26 05:03:34 2019 from workstation.example.com
[batman-redhat.com@workstation-1e37 ~]$ 
----

. Once you chech that you can connect to workstation, become `root` using the provided password:
+
----
$ sudo -i
----

Now that you have accessed the `workstation` machine and become `root`, you can check the rest of the infrastructure.

=== Checking infrastructure status

. Check the status of the infrastructure running the environment, from the `workstation`, using ansible:
+
----
# ansible infra -m ping
----
+
This command establishes a SSH connection to all the infrastructure machines in the environment, defined as `infra` in `/etc/ansible/hosts` which are: vCenter and ESXi servers, storage and workstation, as well as CloudForms. If the machines are being built of booted up, they will show as unreachable. In case the machines are up an running a success message, per each, will show up. 
This is an example of a success message for the VM `cf.example.com`:
+
----
cf.example.com | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
----
+ 
[WARNING]
As this environment is generated and powered up for you in a cloud environment, some resources may suffer from issues or delays depending on the status of the cloud its running on. You may need to wait until everything is up and running, and manually start up or reboot some of them. Follow carefully the upcoming steps to ensure your lab is in a proper running status.

. Let's manually check that CloudForms is running by establishing an SSH connection to it (from workstation) and take a look at `automation.log`:
+
----
[root@workstation-repl ~]# ssh cf
Welcome to the Appliance Console

For a menu, please type: appliance_console
Web console: https://cf.example.com:9090/ or https://192.168.0.100:9090/

Last login: Mon Aug 26 07:08:05 2019 from 192.168.0.10
[root@cf ~]# tail -f /var/www/miq/vmdb/log/automation.log
----
+
[TIP]
The log entries are very long, so it helps if you stretch this window as wide as possible.

Before checking the vSphere environment we have to set up a tunnel to the running environment.

=== Creating an SSH tunnel to the Environment

To access all the resources, from our browser, the same way we would do it being connected directly to the management network, we are going to create an SSH tunnel from our working laptop to the `workstation` machine. This is the diagram of how it will work (explained below):
 
image::ssh_tunnel.png[SSH tunnel for Proxy]

* Our `laptop` connects to `workstation` via ssh
* SSH is instructed to listen on `localhost:3128` in the `laptop`
* SSH takes all the traffic from `localhost:3128` in the `laptop` to `localhost:3128` in the `workstation`
* There is a squid proxy service listening in `localhost:3128` in the `workstation`
* The browser in the `laptop` is configured to use the proxy in `localhost:3128` ... and all the traffic will be sent to the squid proxy in the `workstation`, including the DNS queries. 
* The browser can point now to any service using the internal name (i.e. https://vcenter.example.com ) ... let's do it!

Time to move ahead.

. Let's fire up SSH in your workstation but this time with the "tunnel" option `-L localhost:3128:localhost:3128`
+
----
$ ssh -i /path/to/private_key -L localhost:3128:localhost:3128 <YOUR-OpenTLC-USERNAME-redhat.com>@workstation-<YOUR-GUID>.rhpds.opentlc.com
----

.  Running it shall simply provide a shell prompt. This an example on how it would look like:
+
----
$ ssh -i ~/.ssh/id_rsa -L localhost:3128:localhost:3128 batman-redhat.com@workstation-1e37.rhpds.opentlc.com
Last login: Mon Aug 26 05:03:34 2019 from workstation.example.com
[batman-redhat.com@workstation-1e37 ~]$ 
----

. Now by configuring the browser to access proxy in `localhost` port `3128` for all protocols, we will be running it as if it was directly inside the environment, consumin the internal DNS names. This is an example for Firefox
+
image::localhost_proxy_config.png[Localhost Proxy Config]

. Time to point our browser to an internal URL ... http://vcenter.example.com
+
image::firefox_ssh_tunnel_vcenter.png[Firefox accessing vCenter using a tunnel]
+
[WARNING]
If you haven't managed to make this work, please do not hesitate asking for help. It will be key to proceed with the rest of the lab.

=== Checking vSphere status

. The vSphere environment has been instantiated for us in the cloud and we will be using *nested virtualization* so, the performance may not be as good as in a full baremetal environment. Let's login in the WebUI:

image::vsphere_checks_01.png[Access vCenter UI]

. Use the administrator username which is `administrator@vsphere.local` and the provided password.

image::vsphere_checks_02.png[Login in SSO UI]

. Once in the vCenter UI, click on the *Hosts and Clusters* icon (1), then select *VMCluster* (2) and last, *Reset to Green* all the warnings shown

image::vsphere_checks_03.png[Reset to Green]

. VMs should have been started by the `start_vms` script in `workstation`. In case some VMs are stopped you may manually start them by selecting it and clicking on the *play* icon

image::vsphere_checks_04.png[Press play]

Now the environment is ready to move ahead. We can verify that the Ticket Monster app is running:

* Point your browser to https://app-<YOUR-GUID>.rhpds.opentlc.com or http://app.example.com and check it is running:
+
image::app-ticketmonster-running.png[Ticket Monster app running]
[NOTE]
You must accept all of the self-signed SSL certificates.
+
image::ssl_cert_warning.png[SSL Cert Warning]

Prepare to manage the environment. From a web browser, open each of the URLs below in its own window or tab, using these credentials (except when noted):

* *Username*: `admin`
* *Password*: `<to_be_provided>`
+
[NOTE]
You must accept all of the self-signed SSL certificates.
+

